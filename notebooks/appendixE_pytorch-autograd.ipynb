{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/572_banner.png)\n",
    "%%HTML\n",
    "<script src=\"require.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix E: Auto Differentiation\n",
    "\n",
    "**Mahmood Amintoosi, Spring 2024**\n",
    "\n",
    "Computer Science Dept, Ferdowsi University of Mashhad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explain how the auto-differentiation module of PyTorch works. \n",
    "This module is named **Autograd**.\n",
    "\n",
    "We will first present you how you can compute gradient using PyTorch for a specific variable and how to check the value of the gradient. Then we will use the **backward** function to do the gradient computation. Finally, we will see how to detach a tensor from its computation history and how to tell PyTorch not to keep track of the operations (useful in inference!).\n",
    "\n",
    "More advanced autograd functions are also explained, but we won't go through them during the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Autograd works ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do operations on Tensors, PyTorch can keep track of the computation graph in order to be able to backpropagate.\n",
    "To tell PyTorch to record operations performed on a tensor, each tensor has a function called **`requires_grad_`**.\n",
    "\n",
    "If there’s at least one input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it. Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients.\n",
    "\n",
    "Inplace operations are non-differentiable. That is why `x.zero_()` gives an error if x requires gradient computation.\n",
    "\n",
    "For a tensor x, the underlying data is stored in a tensor that is accessible via **x.data**. If you do an operation on x.data PyTorch does not add the operation to the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a property **`requires_grad`** specifying whether the gradient should be computed during backward pass.\n",
    "\n",
    "The function **`requires_grad_(bool)`** (notice the trailing **\\_** ) is used to change this property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :  tensor([[3., 1.]])\n",
      "A.requires_grad : False\n",
      "A.requires_grad : True\n",
      "A.requires_grad : False\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(10, (1,2), dtype=torch.float)\n",
    "print(\"A : \", A)\n",
    "\n",
    "print(\"A.requires_grad :\", A.requires_grad)\n",
    "\n",
    "A.requires_grad_(True)\n",
    "print(\"A.requires_grad :\", A.requires_grad)\n",
    "\n",
    "A.requires_grad_(False)\n",
    "print(\"A.requires_grad :\", A.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Backward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see a simple example of how to compute the gradient of a function automatically with pytorch.\n",
    "We will check that it correspond to what we can compute manually.\n",
    "\n",
    "Let's look at the function $f(x) = 5x^2+3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x| x=4 : 40.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([4])\n",
    "x.requires_grad_()\n",
    "\n",
    "f = 5 * x ** 2 + 3\n",
    "\n",
    "f.backward()\n",
    "print(\"∂f/∂x| x=4 :\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at the function $f(x) = 5x^2+3sin(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x| x=4 : 40.0\n",
      "∂f/∂y| y=0 : 3.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([4])\n",
    "x.requires_grad_()\n",
    "y = torch.Tensor([0])\n",
    "y.requires_grad_()\n",
    "\n",
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "\n",
    "f.backward()\n",
    "print(\"∂f/∂x| x=4 :\", x.grad.item())\n",
    "print(\"∂f/∂y| y=0 :\", y.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x| x=4 : 40.0\n",
      "∂f/∂y| y=0 : 3.0\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "x = torch.Tensor([4])\n",
    "x.requires_grad_()\n",
    "y = torch.Tensor([0])\n",
    "y.requires_grad_()\n",
    "\n",
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "\n",
    "df_dx = autograd.grad(f, x)[0]\n",
    "print(\"∂f/∂x| x=4 :\", df_dx.item())\n",
    "\n",
    "df_dy = autograd.grad(f, y)[0]\n",
    "print(\"∂f/∂y| y=0 :\", df_dy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(tensor([40.]),)\n",
      "<class 'torch.Tensor'>\n",
      "tensor([40.])\n"
     ]
    }
   ],
   "source": [
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "print(type(autograd.grad(f, x)))\n",
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "print(autograd.grad(f, x))\n",
    "\n",
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "print(type(autograd.grad(f, x)[0]))\n",
    "f = 5 * x**2 + 3*torch.sin(y)\n",
    "print(autograd.grad(f, x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Derivative\n",
    "use create_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x| x=4 : 40.0\n",
      "∂2f/∂x2| x=4 : 10.0\n",
      "∂f/∂y| y=1 : 80.0\n",
      "∂2f/∂y2| y=1 : None\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([4])\n",
    "x.requires_grad_()\n",
    "y = torch.Tensor([1])\n",
    "y.requires_grad_()\n",
    "\n",
    "f = 5 * x**2 * y \n",
    "\n",
    "df_dx = autograd.grad(f, x, create_graph=True)[0]\n",
    "print(\"∂f/∂x| x=4 :\", df_dx.item())\n",
    "d2f_dx2 = autograd.grad(df_dx, x)[0]\n",
    "print(\"∂2f/∂x2| x=4 :\", d2f_dx2.item())\n",
    "\n",
    "df_dy = autograd.grad(f, y, create_graph=True)[0]\n",
    "print(\"∂f/∂y| y=1 :\", df_dy.item())\n",
    "\n",
    "# allow_unused=True\n",
    "d2f_dy2 = autograd.grad(df_dy, y, allow_unused=True)[0]\n",
    "# print(\"∂2f/∂y2| y=1 :\", d2f_dy2.item()) # Error\n",
    "print(\"∂2f/∂y2| y=1 :\", d2f_dy2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the function $f(x, y) = \\sin\\big( \\langle x , y \\rangle \\big)$\n",
    "\n",
    "which is equal to $\\sin(\\sum_i x_iy_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = tensor(0.2964, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([1, 2, 3]).requires_grad_(True)\n",
    "Y = torch.Tensor([5, 6, 7]).requires_grad_(True)\n",
    "\n",
    "f = torch.sin(torch.dot(X,Y))\n",
    "print(\"f =\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply need to call the __backward__ function on $f$.\n",
    "\n",
    "The __backward__ function will automatically compute all the gradients of $f$ wrt. the inputs using the chain rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Backward --\n",
      "\n",
      "X.grad : tensor([4.7754, 5.7304, 6.6855])\n",
      "Manual Derivative: tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n",
      "Y.grad : tensor([0.9551, 1.9101, 2.8652])\n"
     ]
    }
   ],
   "source": [
    "# Gradient is populated by the backward function\n",
    "\n",
    "f.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"X.grad :\", X.grad)\n",
    "print(\"Manual Derivative:\", Y*torch.cos(torch.dot(X,Y)))\n",
    "print(\"Y.grad :\", Y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x : tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n",
      "∂f/∂y: tensor([0.9551, 1.9101, 2.8652])\n"
     ]
    }
   ],
   "source": [
    "f = torch.sin(torch.dot(X,Y))\n",
    "\n",
    "df_dx = autograd.grad(f, X, create_graph=True)[0]\n",
    "print(\"∂f/∂x :\", df_dx)\n",
    "\n",
    "df_dy = autograd.grad(f, Y)[0]\n",
    "print(\"∂f/∂y:\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's compute it manually !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $f$ can be written as a composite function $f = h \\circ g$\n",
    "\n",
    "  $h(z) = \\sin(z)$ with derivative $\\dfrac{d h}{d z}(z) = \\cos(z)$\n",
    "\n",
    "  $g(x, y) =  \\langle x , y \\rangle$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that:  $\\dfrac{\\partial }{\\partial x}(x^Ty) = \\dfrac{\\partial }{\\partial x}(y^Tx) = y$<br> See <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\">Wikipedia: Matrix Calculus</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Using the chain rule, we can easily get the derivative of $f(x, y) = \\sin\\big( \\langle x , y \\rangle \\big)$ w.r.t. $x$ and $y$:\n",
    "\n",
    "  $\\dfrac{d f }{d x} (x,y) = \\cos\\big( \\langle x , y \\rangle \\big) \\cdot y $\n",
    "\n",
    "  and\n",
    "\n",
    "  $\\dfrac{d f }{d y} (x,y) = \\cos\\big( \\langle x , y \\rangle \\big) \\cdot x $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df / dx =  tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "df_dx_man = torch.cos(torch.dot(X,Y)) * Y\n",
    "print(\"df / dx = \", df_dx_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df / dy =  tensor([0.9551, 1.9101, 2.8652], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "df_dy_man = torch.cos(torch.dot(X,Y)) * X\n",
    "print(\"df / dy = \", df_dy_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n",
      "tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(df_dx)\n",
    "print(df_dx_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaf Variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable that __was created by the user__ and was therefore not the result of _any_ operation is called a **leaf variable**.  \n",
    "All variables that have the __`requires_grad` property to False__ are also considered as **leaf variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.is_leaf : True\n",
      "B.is_leaf : False\n",
      "C.is_leaf : False\n",
      "D.is_leaf : True\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]]).requires_grad_()\n",
    "B = torch.Tensor([[1, 2], [3, 4]]).requires_grad_() + 2  # B is the result of an operation (+)\n",
    "C = 5 * A  # C is the result of an operation (*)\n",
    "D = torch.Tensor([[1, 2], [3, 4]])\n",
    "print(\"A.is_leaf :\", A.is_leaf)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "print(\"C.is_leaf :\", C.is_leaf)\n",
    "print(\"D.is_leaf :\", D.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable can have a long computation history, but you may want to consider it as a __new leaf variable__ without history.\n",
    "\n",
    "For that, you can use the `detach` function, which detaches the tensor from its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B :  tensor(1.5000, grad_fn=<MeanBackward0>)\n",
      "B.grad_fn : <MeanBackward0 object at 0x000002150CEA32C8>\n",
      "B.is_leaf : False\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([1, 2]).requires_grad_()\n",
    "B = A.mean()\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.grad_fn :\", B.grad_fn)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "B.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- B.detach_() -- \n",
      "\n",
      "B :  tensor(1.5000)\n",
      "B.grad_fn : None\n",
      "B.is_leaf : True\n"
     ]
    }
   ],
   "source": [
    "B.detach_()\n",
    "print(\"\\n -- B.detach_() -- \\n\")\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.grad_fn :\", B.grad_fn)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "# This won't work since B has no history.\n",
    "# B.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No_grad function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time, you don't want Pytorch to build a computation graph. \n",
    "This can be achieved by wrapping your inference code into the __`with torch.no_grad()`__ context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad :  True\n",
      "y.requires_grad :  True\n",
      "y.requires_grad :  False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x.requires_grad : \", x.requires_grad)\n",
    "\n",
    "y = (x ** 2)\n",
    "print(\"y.requires_grad : \", y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = (x ** 2)\n",
    "    print(\"y.requires_grad : \", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Autograd in previous PyTorch versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In older versions of PyTorch, one had to wrap a `Tensor` into a Autograd object called `Variable`.\n",
    "\n",
    "`Variable` was a thin wrapper around a `Tensor` object, that also held the gradient w.r.t. to it, and a reference to a function that created it. This reference allowed retracing the whole chain of operations that created the data.\n",
    "\n",
    "**Now, `Tensors` are by default `Variable` and we don't need to worry about this anymore**, but you may still encounter it in some \"old\" code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "\n",
    "# x = Variable(torch.randn(5, 5))\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced concepts of Autograd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following concepts are more advanced and may want to skip it for now.  \n",
    "We won't go through them, but there are here for you to come back to later when you feel more comfortable with pytorch.  \n",
    "You can also check the [Pytorch Doc](https://pytorch.org/docs/stable/autograd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain Grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the backward pass, Autograd computes the gradient of the output with respect to every intermediate variables. However, by default, only gradients of variables that were **created by the user** (leaf) and have the __`requires_grad` property to True__ are saved.\n",
    "\n",
    "Indeed, most of the time when training a model you only need the gradient of a loss w.r.t. to your model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂f/∂x| x=4 : 40.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([4])\n",
    "x.requires_grad_()\n",
    "\n",
    "f = 5 * x ** 2 + 3\n",
    "\n",
    "f.backward()\n",
    "print(\"∂f/∂x| x=4 :\", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "B.grad : None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs\\Anaconda3\\envs\\ptch\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "\n",
    "# print(\"A.grad :\", A.grad)\n",
    "# print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad : None\n",
      "B.grad : None\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "B.grad : tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "B.retain_grad()  # <----- This line let us have access to gradient wrt. B after the backward pass\n",
    "C = B.mean()\n",
    "\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can backward a first time and get a gradient for A, then do some other computation using A and then backward again.  \n",
    "Gradients will get accumulated in A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad : None\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[2.5000, 2.5000],\n",
      "        [2.5000, 2.5000]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]]).requires_grad_()\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "C.backward()\n",
    "\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "C.backward()\n",
    "\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to give a glimpse of how it works under the hood. We don't need to do such inspection in practice.  \n",
    "Here, we have a look at the computation graph that autograd builds on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + A)\n",
    "C = B.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<MulBackward0 object at 0x000001565DBD7348>\n",
      "<MeanBackward0 object at 0x000001565DBD7388>\n"
     ]
    }
   ],
   "source": [
    "print(A.grad_fn)\n",
    "print(B.grad_fn)\n",
    "print(C.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also \"walk\" on the computation graph by calling the `next_functions` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x000001565DBC4788>\n",
      "((<MulBackward0 object at 0x000001565DBC4948>, 0),)\n",
      "((<AddBackward0 object at 0x000001565DBC4788>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x000001565DBC4948>, 0), (<AccumulateGrad object at 0x000001565DBC4948>, 0))\n"
     ]
    }
   ],
   "source": [
    "grad_fn = C.grad_fn\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn.next_functions\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn[0][0].next_functions\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn[0][0].next_functions\n",
    "print(grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
