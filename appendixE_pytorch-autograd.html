
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Appendix E: Auto Differentiation &#8212; NN</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'appendixE_pytorch-autograd';</script>
    <link rel="icon" href="_static/fum-logo.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Appendix F: Optimization" href="appendixF_pytorch-optimizers.html" />
    <link rel="prev" title="Appendix D: Basics of Numpy and Tensors" href="appendixD_tensor-basics.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fum-cs-logo.png" class="logo__image only-light" alt="NN - Home"/>
    <script>document.write(`<img src="_static/fum-cs-logo.png" class="logo__image only-dark" alt="NN - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Neural Networks
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture0_introduction.html">Introduction to the Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture1_floating-point.html">Floating Point Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture2_1_Regression.html">Regression</a></li>



<li class="toctree-l1"><a class="reference internal" href="lecture2_2_gradient-descent.html">Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture3_stochastic-gradient-descent.html">Stochastic Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture4_pytorch-neural-networks-pt1.html">Introduction to Pytorch &amp; Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture5_neural-networks-pt2.html">Training Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture6_cnns-pt1.html">Introduction to Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture7_cnns-pt2.html">Advanced Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendixA_gradients.html">Appendix A: Gradients Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendixB_logistic-loss.html">Appendix B: Logistic Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendixC_computing-derivatives.html">Appendix C: Computing Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendixD_tensor-basics.html">Appendix D: Basics of Numpy and Tensors</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Appendix E: Auto Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendixF_pytorch-optimizers.html">Appendix F: Optimization</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/fum-cs/neural-networks/blob/main/notebooks/appendixE_pytorch-autograd.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/fum-cs/neural-networks" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/fum-cs/neural-networks/issues/new?title=Issue%20on%20page%20%2FappendixE_pytorch-autograd.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/appendixE_pytorch-autograd.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Appendix E: Auto Differentiation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-autograd-works">How does Autograd works ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-requires-grad">Function requires_grad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-function">Backward function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-autograd-grad">Using autograd.grad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-derivative">Second Derivative</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#now-let-s-compute-it-manually">Now let’s compute it manually !</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaf-variable">Leaf Variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detach-function">Detach function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-grad-function">No_grad function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-autograd-in-previous-pytorch-versions">Note: Autograd in previous PyTorch versions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-concepts-of-autograd">Advanced concepts of Autograd</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retain-grad">Retain Grad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation">Gradient accumulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#under-the-hood">Under the hood…</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><img alt="" src="_images/572_banner.png" /></p>
<script src="require.js"></script><section class="tex2jax_ignore mathjax_ignore" id="appendix-e-auto-differentiation">
<h1>Appendix E: Auto Differentiation<a class="headerlink" href="#appendix-e-auto-differentiation" title="Link to this heading">#</a></h1>
<p><strong>Mahmood Amintoosi, Spring 2024</strong></p>
<p>Computer Science Dept, Ferdowsi University of Mashhad</p>
<p>In this notebook, we will explain how the auto-differentiation module of PyTorch works.
This module is named <strong>Autograd</strong>.</p>
<p>We will first present you how you can compute gradient using PyTorch for a specific variable and how to check the value of the gradient. Then we will use the <strong>backward</strong> function to do the gradient computation. Finally, we will see how to detach a tensor from its computation history and how to tell PyTorch not to keep track of the operations (useful in inference!).</p>
<p>More advanced autograd functions are also explained, but we won’t go through them during the workshop.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;1.12.1+cpu&#39;
</pre></div>
</div>
</div>
</div>
<section id="how-does-autograd-works">
<h2>How does Autograd works ?<a class="headerlink" href="#how-does-autograd-works" title="Link to this heading">#</a></h2>
<p>When you do operations on Tensors, PyTorch can keep track of the computation graph in order to be able to backpropagate.
To tell PyTorch to record operations performed on a tensor, each tensor has a function called <strong><code class="docutils literal notranslate"><span class="pre">requires_grad_</span></code></strong>.</p>
<p>If there’s at least one input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it. Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients.</p>
<p>Inplace operations are non-differentiable. That is why <code class="docutils literal notranslate"><span class="pre">x.zero_()</span></code> gives an error if x requires gradient computation.</p>
<p>For a tensor x, the underlying data is stored in a tensor that is accessible via <strong>x.data</strong>. If you do an operation on x.data PyTorch does not add the operation to the computation graph.</p>
</section>
<section id="function-requires-grad">
<h2>Function requires_grad<a class="headerlink" href="#function-requires-grad" title="Link to this heading">#</a></h2>
<p>Each tensor has a property <strong><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></strong> specifying whether the gradient should be computed during backward pass.</p>
<p>The function <strong><code class="docutils literal notranslate"><span class="pre">requires_grad_(bool)</span></code></strong> (notice the trailing <strong>_</strong> ) is used to change this property.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A : &quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.requires_grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">A</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.requires_grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">A</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.requires_grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A :  tensor([[3., 1.]])
A.requires_grad : False
A.requires_grad : True
A.requires_grad : False
</pre></div>
</div>
</div>
</div>
</section>
<section id="backward-function">
<h2>Backward function<a class="headerlink" href="#backward-function" title="Link to this heading">#</a></h2>
<p>Here we will see a simple example of how to compute the gradient of a function automatically with pytorch.
We will check that it correspond to what we can compute manually.</p>
<p>Let’s look at the function <span class="math notranslate nohighlight">\(f(x) = 5x^2+3\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x| x=4 :&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x| x=4 : 40.0
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the function <span class="math notranslate nohighlight">\(f(x) = 5x^2+3sin(y)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x| x=4 :&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂y| y=0 :&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x| x=4 : 40.0
∂f/∂y| y=0 : 3.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-autograd-grad">
<h2>Using autograd.grad<a class="headerlink" href="#using-autograd-grad" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autograd</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">df_dx</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x| x=4 :&quot;</span><span class="p">,</span> <span class="n">df_dx</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">df_dy</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂y| y=0 :&quot;</span><span class="p">,</span> <span class="n">df_dy</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x| x=4 : 40.0
∂f/∂y| y=0 : 3.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)))</span>
<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;tuple&#39;&gt;
(tensor([40.]),)
&lt;class &#39;torch.Tensor&#39;&gt;
tensor([40.])
</pre></div>
</div>
</div>
</div>
</section>
<section id="second-derivative">
<h2>Second Derivative<a class="headerlink" href="#second-derivative" title="Link to this heading">#</a></h2>
<p>use create_graph=True</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y</span> 

<span class="n">df_dx</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x| x=4 :&quot;</span><span class="p">,</span> <span class="n">df_dx</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">d2f_dx2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">df_dx</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂2f/∂x2| x=4 :&quot;</span><span class="p">,</span> <span class="n">d2f_dx2</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">df_dy</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂y| y=1 :&quot;</span><span class="p">,</span> <span class="n">df_dy</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="c1"># allow_unused=True</span>
<span class="n">d2f_dy2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">df_dy</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># print(&quot;∂2f/∂y2| y=1 :&quot;, d2f_dy2.item()) # Error</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂2f/∂y2| y=1 :&quot;</span><span class="p">,</span> <span class="n">d2f_dy2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x| x=4 : 40.0
∂2f/∂x2| x=4 : 10.0
∂f/∂y| y=1 : 80.0
∂2f/∂y2| y=1 : None
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the function <span class="math notranslate nohighlight">\(f(x, y) = \sin\big( \langle x , y \rangle \big)\)</span></p>
<p>which is equal to <span class="math notranslate nohighlight">\(\sin(\sum_i x_iy_i)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f =&quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>f = tensor(0.2964, grad_fn=&lt;SinBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We simply need to call the <strong>backward</strong> function on <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>The <strong>backward</strong> function will automatically compute all the gradients of <span class="math notranslate nohighlight">\(f\)</span> wrt. the inputs using the chain rule!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient is populated by the backward function</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-- Backward --</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X.grad :&quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manual Derivative:&quot;</span><span class="p">,</span> <span class="n">Y</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Y.grad :&quot;</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-- Backward --

X.grad : tensor([4.7754, 5.7304, 6.6855])
Manual Derivative: tensor([4.7754, 5.7304, 6.6855], grad_fn=&lt;MulBackward0&gt;)
Y.grad : tensor([0.9551, 1.9101, 2.8652])
</pre></div>
</div>
</div>
</div>
<p>And by autograd.grad</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="n">df_dx</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x :&quot;</span><span class="p">,</span> <span class="n">df_dx</span><span class="p">)</span>

<span class="n">df_dy</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂y:&quot;</span><span class="p">,</span> <span class="n">df_dy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x : tensor([4.7754, 5.7304, 6.6855], grad_fn=&lt;MulBackward0&gt;)
∂f/∂y: tensor([0.9551, 1.9101, 2.8652])
</pre></div>
</div>
</div>
</div>
<section id="now-let-s-compute-it-manually">
<h3>Now let’s compute it manually !<a class="headerlink" href="#now-let-s-compute-it-manually" title="Link to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(f\)</span> can be written as a composite function <span class="math notranslate nohighlight">\(f = h \circ g\)</span></p>
<p><span class="math notranslate nohighlight">\(h(z) = \sin(z)\)</span> with derivative <span class="math notranslate nohighlight">\(\dfrac{d h}{d z}(z) = \cos(z)\)</span></p>
<p><span class="math notranslate nohighlight">\(g(x, y) =  \langle x , y \rangle\)</span></p>
</li>
</ul>
<p>We know that:  <span class="math notranslate nohighlight">\(\dfrac{\partial }{\partial x}(x^Ty) = \dfrac{\partial }{\partial x}(y^Tx) = y\)</span><br> See <a href="https://en.wikipedia.org/wiki/Matrix_calculus">Wikipedia: Matrix Calculus</a></p>
<ul class="simple">
<li><p>Using the chain rule, we can easily get the derivative of <span class="math notranslate nohighlight">\(f(x, y) = \sin\big( \langle x , y \rangle \big)\)</span> w.r.t. <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\dfrac{d f }{d x} (x,y) = \cos\big( \langle x , y \rangle \big) \cdot y \)</span></p>
<p>and</p>
<p><span class="math notranslate nohighlight">\(\dfrac{d f }{d y} (x,y) = \cos\big( \langle x , y \rangle \big) \cdot x \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_dx_man</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span> <span class="o">*</span> <span class="n">Y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df / dx = &quot;</span><span class="p">,</span> <span class="n">df_dx_man</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>df / dx =  tensor([4.7754, 5.7304, 6.6855], grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_dy_man</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span> <span class="o">*</span> <span class="n">X</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;df / dy = &quot;</span><span class="p">,</span> <span class="n">df_dy_man</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>df / dy =  tensor([0.9551, 1.9101, 2.8652], grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df_dx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_dx_man</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.7754, 5.7304, 6.6855], grad_fn=&lt;MulBackward0&gt;)
tensor([4.7754, 5.7304, 6.6855], grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Success !</p>
</section>
<section id="leaf-variable">
<h3>Leaf Variable<a class="headerlink" href="#leaf-variable" title="Link to this heading">#</a></h3>
<p>A variable that <strong>was created by the user</strong> and was therefore not the result of <em>any</em> operation is called a <strong>leaf variable</strong>.<br />
All variables that have the <strong><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> property to False</strong> are also considered as <strong>leaf variable</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span>  <span class="c1"># B is the result of an operation (+)</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">A</span>  <span class="c1"># C is the result of an operation (*)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.is_leaf :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.is_leaf :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;C.is_leaf :&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;D.is_leaf :&quot;</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A.is_leaf : True
B.is_leaf : False
C.is_leaf : False
D.is_leaf : True
</pre></div>
</div>
</div>
</div>
</section>
<section id="detach-function">
<h3>Detach function<a class="headerlink" href="#detach-function" title="Link to this heading">#</a></h3>
<p>A variable can have a long computation history, but you may want to consider it as a <strong>new leaf variable</strong> without history.</p>
<p>For that, you can use the <code class="docutils literal notranslate"><span class="pre">detach</span></code> function, which detaches the tensor from its history.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B : &quot;</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.grad_fn :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.is_leaf :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
<span class="n">B</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>B :  tensor(1.5000, grad_fn=&lt;MeanBackward0&gt;)
B.grad_fn : &lt;MeanBackward0 object at 0x000002150CEA32C8&gt;
B.is_leaf : False
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">B</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> -- B.detach_() -- </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B : &quot;</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.grad_fn :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.is_leaf :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">)</span>
<span class="c1"># This won&#39;t work since B has no history.</span>
<span class="c1"># B.backward()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> -- B.detach_() -- 

B :  tensor(1.5000)
B.grad_fn : None
B.is_leaf : True
</pre></div>
</div>
</div>
</div>
</section>
<section id="no-grad-function">
<h3>No_grad function<a class="headerlink" href="#no-grad-function" title="Link to this heading">#</a></h3>
<p>At inference time, you don’t want Pytorch to build a computation graph.
This can be achieved by wrapping your inference code into the <strong><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad()</span></code></strong> context manager.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.requires_grad : &quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.requires_grad : &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y.requires_grad : &quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x.requires_grad :  True
y.requires_grad :  True
y.requires_grad :  False
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="note-autograd-in-previous-pytorch-versions">
<h2>Note: Autograd in previous PyTorch versions<a class="headerlink" href="#note-autograd-in-previous-pytorch-versions" title="Link to this heading">#</a></h2>
<p>In older versions of PyTorch, one had to wrap a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> into a Autograd object called <code class="docutils literal notranslate"><span class="pre">Variable</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">Variable</span></code> was a thin wrapper around a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object, that also held the gradient w.r.t. to it, and a reference to a function that created it. This reference allowed retracing the whole chain of operations that created the data.</p>
<p><strong>Now, <code class="docutils literal notranslate"><span class="pre">Tensors</span></code> are by default <code class="docutils literal notranslate"><span class="pre">Variable</span></code> and we don’t need to worry about this anymore</strong>, but you may still encounter it in some “old” code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># from torch.autograd import Variable</span>

<span class="c1"># x = Variable(torch.randn(5, 5))</span>
<span class="c1"># x</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="advanced-concepts-of-autograd">
<h2>Advanced concepts of Autograd<a class="headerlink" href="#advanced-concepts-of-autograd" title="Link to this heading">#</a></h2>
<p>The following concepts are more advanced and may want to skip it for now.<br />
We won’t go through them, but there are here for you to come back to later when you feel more comfortable with pytorch.<br />
You can also check the <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">Pytorch Doc</a>.</p>
<section id="retain-grad">
<h3>Retain Grad<a class="headerlink" href="#retain-grad" title="Link to this heading">#</a></h3>
<p>When doing the backward pass, Autograd computes the gradient of the output with respect to every intermediate variables. However, by default, only gradients of variables that were <strong>created by the user</strong> (leaf) and have the <strong><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> property to True</strong> are saved.</p>
<p>Indeed, most of the time when training a model you only need the gradient of a loss w.r.t. to your model parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">f</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;∂f/∂x| x=4 :&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>∂f/∂x| x=4 : 40.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">A</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># print(&quot;A.grad :&quot;, A.grad)</span>
<span class="c1"># print(&quot;B.grad :&quot;, B.grad)</span>
<span class="n">C</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-- Backward --</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.grad :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-- Backward --

A.grad : tensor([[1.2500, 1.2500],
        [1.2500, 1.2500]])
B.grad : None
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Programs\Anaconda3\envs\ptch\lib\site-packages\torch\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#39;t be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\src\ATen/core/TensorBody.h:417.)
  return self._grad
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">A</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">B</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>  <span class="c1"># &lt;----- This line let us have access to gradient wrt. B after the backward pass</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.grad :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">C</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-- Backward --</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;B.grad :&quot;</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A.grad : None
B.grad : None

-- Backward --

A.grad : tensor([[1.2500, 1.2500],
        [1.2500, 1.2500]])
B.grad : tensor([[0.2500, 0.2500],
        [0.2500, 0.2500]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="gradient-accumulation">
<h3>Gradient accumulation<a class="headerlink" href="#gradient-accumulation" title="Link to this heading">#</a></h3>
<p>You can backward a first time and get a gradient for A, then do some other computation using A and then backward again.<br />
Gradients will get accumulated in A.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-- Backward --</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-- Backward --</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A.grad :&quot;</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A.grad : None

-- Backward --

A.grad : tensor([[1.2500, 1.2500],
        [1.2500, 1.2500]])

-- Backward --

A.grad : tensor([[2.5000, 2.5000],
        [2.5000, 2.5000]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="under-the-hood">
<h3>Under the hood…<a class="headerlink" href="#under-the-hood" title="Link to this heading">#</a></h3>
<p>This part is to give a glimpse of how it works under the hood. We don’t need to do such inspection in practice.<br />
Here, we have a look at the computation graph that autograd builds on the fly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">A</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="n">A</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Each tensor has a gradient function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>None
&lt;MulBackward0 object at 0x000001565DBD7348&gt;
&lt;MeanBackward0 object at 0x000001565DBD7388&gt;
</pre></div>
</div>
</div>
</div>
<p>We can also “walk” on the computation graph by calling the <code class="docutils literal notranslate"><span class="pre">next_functions</span></code> attribute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grad_fn</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">grad_fn</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">next_functions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">next_functions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;MeanBackward0 object at 0x000001565DBC4788&gt;
((&lt;MulBackward0 object at 0x000001565DBC4948&gt;, 0),)
((&lt;AddBackward0 object at 0x000001565DBC4788&gt;, 0), (None, 0))
((&lt;AccumulateGrad object at 0x000001565DBC4948&gt;, 0), (&lt;AccumulateGrad object at 0x000001565DBC4948&gt;, 0))
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="appendixD_tensor-basics.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Appendix D: Basics of Numpy and Tensors</p>
      </div>
    </a>
    <a class="right-next"
       href="appendixF_pytorch-optimizers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Appendix F: Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-autograd-works">How does Autograd works ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-requires-grad">Function requires_grad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-function">Backward function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-autograd-grad">Using autograd.grad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-derivative">Second Derivative</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#now-let-s-compute-it-manually">Now let’s compute it manually !</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leaf-variable">Leaf Variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detach-function">Detach function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-grad-function">No_grad function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note-autograd-in-previous-pytorch-versions">Note: Autograd in previous PyTorch versions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-concepts-of-autograd">Advanced concepts of Autograd</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retain-grad">Retain Grad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation">Gradient accumulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#under-the-hood">Under the hood…</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mahmood Amintoosi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024. CC0 Licensed - Computer Science Dept., Ferdowsi University of Mashhad.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>